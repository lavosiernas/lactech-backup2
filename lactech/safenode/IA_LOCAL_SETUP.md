# ü§ñ Configura√ß√£o de IA Local (100% Gratuita!)

N√£o quer gastar com APIs pagas? Configure uma IA local! Totalmente gr√°tis e roda na sua m√°quina.

---

## üöÄ Op√ß√£o 1: Ollama (Recomendado - Mais F√°cil)

Ollama √© a forma mais simples de rodar modelos de IA localmente.

### üì• Instala√ß√£o

#### Windows:
1. Baixe em: https://ollama.com/download/windows
2. Execute o instalador
3. Pronto! Ollama j√° est√° rodando

#### Linux/Mac:
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### üéØ Configura√ß√£o no SafeNode

1. **Baixe um modelo pequeno e r√°pido** (recomendado para come√ßar):
```bash
ollama pull llama3.2:1b
```
ou modelos maiores (melhor qualidade, mais lento):
```bash
ollama pull llama3.2:3b
ollama pull mistral:7b
```

2. **Configure no SafeNode** (j√° est√° configurado por padr√£o!):
   - O sistema j√° detecta Ollama em `http://localhost:11434`
   - Configure `AI_PROVIDER=local` (ou deixe padr√£o)

3. **Pronto!** Use o assistente de IA normalmente.

### üìã Comandos √öteis do Ollama

```bash
# Listar modelos instalados
ollama list

# Rodar um modelo manualmente (teste)
ollama run llama3.2:1b "Como criar um template HTML?"

# Remover um modelo
ollama rm nome-do-modelo
```

---

## üé® Op√ß√£o 2: LM Studio

Interface gr√°fica bonita para gerenciar modelos de IA.

### üì• Instala√ß√£o:
1. Baixe em: https://lmstudio.ai/
2. Instale e abra
3. Baixe um modelo (ex: Llama 3, Mistral)
4. Configure o servidor local

### ‚öôÔ∏è Configura√ß√£o:
- Abra LM Studio ‚Üí Local Server
- Inicie o servidor na porta 1234
- Configure no SafeNode:
  ```
  LOCAL_AI_URL=http://localhost:1234/v1/chat/completions
  AI_PROVIDER=local
  ```

---

## üîß Op√ß√£o 3: Text Generation WebUI (Oobabooga)

Mais op√ß√µes e controle, mas mais complexo.

1. Instale seguindo: https://github.com/oobabooga/text-generation-webui
2. Configure para rodar na porta padr√£o
3. Use a API local

---

## ‚öôÔ∏è Configura√ß√£o Avan√ßada

### Vari√°veis de Ambiente Opcionais:

```bash
# URL da API local (padr√£o: http://localhost:11434/api/generate para Ollama)
LOCAL_AI_URL=http://localhost:11434/api/generate

# Modelo a usar (padr√£o: llama3.2:1b)
LOCAL_AI_MODEL=llama3.2:1b

# Provedor de IA (local usa IA local)
AI_PROVIDER=local
```

### Windows (PowerShell):
```powershell
$env:AI_PROVIDER = "local"
$env:LOCAL_AI_MODEL = "llama3.2:1b"
```

### Linux/Mac:
```bash
export AI_PROVIDER=local
export LOCAL_AI_MODEL=llama3.2:1b
```

---

## üéØ Modelos Recomendados

### Para Come√ßar (Pequenos e R√°pidos):
- **llama3.2:1b** - Muito r√°pido, qualidade b√°sica (~700MB)
- **llama3.2:3b** - Bom equil√≠brio (~2GB)
- **mistral:7b** - Melhor qualidade (~4GB)

### Para Produ√ß√£o (Melhor Qualidade):
- **llama3:8b** - Excelente qualidade (~4.7GB)
- **mistral:7b-instruct** - Otimizado para instru√ß√µes (~4GB)
- **codellama:7b** - Especializado em c√≥digo (~3.8GB)

---

## ‚úÖ Testando

1. **Teste se Ollama est√° rodando:**
```bash
curl http://localhost:11434/api/tags
```

2. **Teste um modelo:**
```bash
ollama run llama3.2:1b "Ol√°, voc√™ est√° funcionando?"
```

3. **No SafeNode:**
   - Abra a IDE de c√≥digo
   - Use o assistente de IA
   - Deve funcionar sem APIs pagas!

---

## üêõ Solu√ß√£o de Problemas

### Ollama n√£o est√° rodando:
```bash
# Windows: Procure "Ollama" no menu iniciar e execute
# Linux/Mac:
ollama serve
```

### Porta ocupada:
- Ollama usa porta 11434 por padr√£o
- Altere se necess√°rio: `LOCAL_AI_URL=http://localhost:PORTA/api/generate`

### Modelo n√£o encontrado:
```bash
ollama pull nome-do-modelo
```

### Respostas muito lentas:
- Use modelos menores (1b ou 3b)
- Ou aumente o timeout no c√≥digo

---

## üí° Dicas

1. **Comece pequeno**: Use `llama3.2:1b` primeiro para testar
2. **Upgrade depois**: Se precisar de melhor qualidade, baixe modelos maiores
3. **Sem internet**: IA local funciona 100% offline!
4. **Privacidade**: Seus dados nunca saem da sua m√°quina
5. **Gratuito**: Zero custos, zero limites

---

## üéâ Vantagens da IA Local

‚úÖ **100% Gratuito** - Sem custos  
‚úÖ **Offline** - Funciona sem internet  
‚úÖ **Privacidade** - Dados n√£o saem da sua m√°quina  
‚úÖ **Sem limites** - Use quanto quiser  
‚úÖ **R√°pido** - Sem lat√™ncia de rede (dependendo do hardware)  
‚úÖ **Controle total** - Escolha o modelo que quiser  

---

**Pronto! Agora voc√™ tem IA gratuita rodando localmente! üöÄ**





